{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 加载ET语料库\n",
    "ETm1_Data = pd.read_csv(\"./ETDataset/ETT-small/ETTh1.csv\")\n",
    "ETm1_Data = ETm1_Data.drop([\"date\"], axis=1)  # 删除原始日期列\n",
    "\n",
    "\n",
    "# 使用 scikit-learn 的 MinMaxScaler 进行归一化\n",
    "scaler = MinMaxScaler()\n",
    "ETm1_Data_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(ETm1_Data),\n",
    "    columns=ETm1_Data.columns[:],\n",
    ")\n",
    "\n",
    "# # 下面是带时间戳的代码，如使用，请将transformer-input_size变为8\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # 加载ET语料库\n",
    "# ETm1_Data = pd.read_csv(\"./ETDataset/ETT-small/ETTh1.csv\")\n",
    "\n",
    "\n",
    "# # 转换日期为时间戳\n",
    "# def transform_date_to_timestamp(df):\n",
    "#     df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "#     df[\"timestamp\"] = df[\"date\"].apply(lambda x: x.timestamp())\n",
    "#     df = df.drop([\"date\"], axis=1)  # 删除原始日期列\n",
    "#     return df\n",
    "\n",
    "\n",
    "# ETm1_Data = transform_date_to_timestamp(ETm1_Data)\n",
    "\n",
    "# # 使用 scikit-learn 的 MinMaxScaler 进行归一化(时间戳没有归一化)\n",
    "# scaler = MinMaxScaler()\n",
    "# ETm1_Data_normalized = pd.DataFrame(\n",
    "#     scaler.fit_transform(ETm1_Data.drop([\"timestamp\"], axis=1)),\n",
    "#     columns=ETm1_Data.columns[:-1],\n",
    "# )\n",
    "# ETm1_Data_normalized[\"timestamp\"] = ETm1_Data[\"timestamp\"]\n",
    "\n",
    "# # # 使用 scikit-learn 的 MinMaxScaler 进行归一化(时间戳归一化)\n",
    "# # scaler = MinMaxScaler()\n",
    "# # ETm1_Data_normalized = pd.DataFrame(\n",
    "# #     scaler.fit_transform(ETm1_Data),\n",
    "# #     columns=ETm1_Data.columns[:],\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(Data, Predict_HourLength = 96, Count_PerHour = 1):\n",
    "    # 将数据分组\n",
    "    group_size = Predict_HourLength * Count_PerHour * 2\n",
    "    groups = [\n",
    "        Data.iloc[i : i + group_size]\n",
    "        for i in range(0, len(Data) - group_size + 1)\n",
    "    ]\n",
    "\n",
    "    # 提取特征值和预测值\n",
    "    features = [group.iloc[: Predict_HourLength * Count_PerHour] for group in groups]\n",
    "    labels = [group.iloc[Predict_HourLength * Count_PerHour :] for group in groups]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按比例划分数据集\n",
    "total_samples = len(ETm1_Data)\n",
    "train_size = int(0.6 * total_samples)\n",
    "val_size = int(0.2 * total_samples)\n",
    "\n",
    "train_data = ETm1_Data[:train_size]\n",
    "val_data = ETm1_Data[train_size : train_size + val_size]\n",
    "test_data = ETm1_Data[train_size + val_size :]\n",
    "\n",
    "train_data_normalized = ETm1_Data_normalized[:train_size]\n",
    "val_data_normalized = ETm1_Data_normalized[train_size : train_size + val_size]\n",
    "test_data_normalized = ETm1_Data_normalized[train_size + val_size :]\n",
    "\n",
    "# 按时间步得到数据集\n",
    "train_features, train_labels = get_features_labels(train_data)\n",
    "val_features, val_labels = get_features_labels(val_data)\n",
    "test_features, test_labels = get_features_labels(test_data)\n",
    "train_features_normalized, train_labels_normalized = get_features_labels(train_data_normalized)\n",
    "val_features_normalized, val_labels_normalized = get_features_labels(val_data_normalized)\n",
    "test_features_normalized, test_labels_normalized = get_features_labels(test_data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, features, labels, features_normalized, labels_normalized):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.features_normalized = features_normalized\n",
    "        self.labels_normalized = labels_normalized\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 将数据转换为 PyTorch 张量\n",
    "        feature_tensor = torch.tensor(self.features[index].values, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(self.labels[index].values, dtype=torch.float32)\n",
    "        feature_normalized_tensor = torch.tensor(self.features_normalized[index].values, dtype=torch.float32)\n",
    "        label_normalized_tensor = torch.tensor(self.labels_normalized[index].values, dtype=torch.float32)\n",
    "\n",
    "        return feature_tensor, label_tensor, feature_normalized_tensor, label_normalized_tensor\n",
    "\n",
    "\n",
    "# 创建训练集、验证集和测试集的 Dataset 实例\n",
    "train_dataset = MyDataset(train_features, train_labels, train_features_normalized, train_labels_normalized)\n",
    "val_dataset = MyDataset(val_features, val_labels, val_features_normalized, val_labels_normalized)\n",
    "test_dataset = MyDataset(test_features, test_labels, test_features_normalized, test_labels_normalized)\n",
    "\n",
    "# 创建 DataLoader 实例\n",
    "batch_size = 64  # 批量大小\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\"\n",
    "\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Transformer\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, nhead, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding_layer = nn.Linear(input_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=embed_size, dropout=0)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Linear(embed_size, input_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding_layer(src)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.embedding_layer(tgt)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(\n",
    "            src.device\n",
    "        )\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 7  # 输入特征的数量\n",
    "embed_size = 128\n",
    "nhead = 4\n",
    "num_layers = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TransformerModel(input_size, embed_size, nhead, num_layers)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"it\") as pbar:\n",
    "        # 训练\n",
    "        model.train()\n",
    "        for _, targets, inputs, _ in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # 将右移后的目标序列传递给 Transformer\n",
    "            targets_input = torch.cat([torch.zeros_like(targets[:, :1]), targets[:, :-1]], dim=1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, targets_input)\n",
    "            outputs = model.predictor(outputs)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pbar.set_postfix({\"loss (batch)\": loss.item()})\n",
    "            pbar.update(1)  # 更当前进度，1表示完成了一个batch的训练\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"it\") as pbar:\n",
    "    #     # 训练\n",
    "    #     model.train()\n",
    "    #     for _, targets, inputs, _ in train_loader:\n",
    "    #         inputs, targets = inputs.to(device), targets.to(device)\n",
    "    #         predict_result = torch.zeros((targets.size(0), 1, targets.size(2))).to(device)\n",
    "    #         optimizer.zero_grad()\n",
    "    #         for i in range(targets.size(1)):\n",
    "    #             outputs = model(inputs, predict_result)\n",
    "    #             outputs = model.predictor(outputs)\n",
    "    #             predict_result = torch.cat([predict_result, targets[:, i].unsqueeze(1)], dim=1)\n",
    "            \n",
    "    #         loss = criterion(outputs, targets)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         total_loss += loss.item()\n",
    "            \n",
    "    #         pbar.set_postfix({\"loss (batch)\": loss.item()})\n",
    "    #         pbar.update(1)  # 更当前进度，1表示完成了一个batch的训练\n",
    "\n",
    "    #     scheduler.step()\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        with tqdm(total=len(val_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"it\") as pbar:\n",
    "            # 验证\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for _, val_targets, val_inputs, _ in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    predict_result = torch.zeros((val_targets.size(0), 1, val_targets.size(2))).to(device)\n",
    "                    for i in range(val_targets.size(1)):\n",
    "                        val_outputs = model(val_inputs, predict_result)\n",
    "                        val_outputs = model.predictor(val_outputs)\n",
    "                        predict_result = torch.cat([predict_result, val_outputs[:, -1].unsqueeze(1)], dim=1)\n",
    "\n",
    "                    val_loss = criterion(predict_result[:, 1:], val_targets)\n",
    "                    total_val_loss += val_loss.item()\n",
    "\n",
    "                    pbar.set_postfix({\"val_loss (batch)\": val_loss.item()})\n",
    "                    pbar.update(1)  # 更当前进度，1表示完成了一个batch的验证\n",
    "\n",
    "    average_train_loss = total_loss / len(train_loader.dataset)\n",
    "    if epoch%10 == 0:\n",
    "        average_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, average_train_loss: {average_train_loss}\")\n",
    "    if epoch%10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, average_train_loss: {average_train_loss}, average_val_loss: {average_val_loss}\")\n",
    "\n",
    "    # 保存模型参数\n",
    "    model_state_dict_path = \"./model/transformer_{}.pth\".format(epoch + 1)\n",
    "    torch.save(model.state_dict(), model_state_dict_path)\n",
    "\n",
    "    print(\"Saved model state dict:\", model_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 加载模型参数\n",
    "model_path = \"./model/transformer_91.pth\"\n",
    "model.load_state_dict(torch.load(model_path)) \n",
    "# model = model.to(\"cpu\")\n",
    "\n",
    "total_MAE = 0.0\n",
    "total_MSE = 0.0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, test_targets, test_inputs, _ in test_loader:\n",
    "        test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "        predict_result = torch.zeros((test_targets.size(0), 1, test_targets.size(2))).to(device)\n",
    "        for i in range(test_targets.size(1)):\n",
    "            test_outputs = model(test_inputs, predict_result)\n",
    "            test_outputs = model.predictor(test_outputs)\n",
    "            predict_result = torch.cat([predict_result, test_outputs[:, -1].unsqueeze(1)], dim=1)\n",
    "        predict_result = predict_result[:, 1:]\n",
    "        \n",
    "        # predict_result = torch.zeros_like(test_targets)\n",
    "        # predict_result = model(test_inputs, predict_result)\n",
    "        # predict_result = model.predictor(predict_result)\n",
    "\n",
    "        # predict_result = model(test_inputs, test_inputs)\n",
    "        # predict_result = model.predictor(predict_result)\n",
    "\n",
    "        # predict_result = model(test_inputs, test_targets)\n",
    "        # predict_result = model.predictor(predict_result)\n",
    "\n",
    "        # 计算 MAE\n",
    "        mae_loss = F.l1_loss(predict_result, test_targets, reduction='mean')\n",
    "        total_MAE += mae_loss.item()\n",
    "\n",
    "        # 计算 MSE\n",
    "        mse_loss = F.mse_loss(predict_result, test_targets, reduction='mean')\n",
    "        total_MSE += mse_loss.item()\n",
    "\n",
    "    # 计算平均 MAE 和 MSE\n",
    "    average_MAE = total_MAE / len(test_loader.dataset)\n",
    "    average_MSE = total_MSE / len(test_loader.dataset)\n",
    "    print(f\"Average MAE: {average_MAE}\")\n",
    "    print(f\"Average MSE: {average_MSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载模型参数\n",
    "model_path = \"./model/transformer_91.pth\"\n",
    "model.load_state_dict(torch.load(model_path)) \n",
    "# model = model.to(\"cpu\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, test_targets, test_inputs, _ in train_loader:\n",
    "        test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "        predict_result = torch.zeros((test_targets.size(0), 1, test_targets.size(2))).to(device)\n",
    "        for i in range(test_targets.size(1)):\n",
    "            test_outputs = model(test_inputs, predict_result)\n",
    "            test_outputs = model.predictor(test_outputs)\n",
    "            predict_result = torch.cat([predict_result, test_outputs[:, -1].unsqueeze(1)], dim=1)\n",
    "        predict_result = predict_result[:, 1:]\n",
    "        \n",
    "        # predict_result = torch.zeros_like(test_targets)\n",
    "        # predict_result = model(test_inputs, predict_result)\n",
    "        # predict_result = model.predictor(predict_result)\n",
    "\n",
    "        # predict_result = model(test_inputs, test_inputs)\n",
    "        # predict_result = model.predictor(predict_result)\n",
    "\n",
    "        # predict_result = model(test_inputs, test_targets)\n",
    "        # predict_result = model.predictor(predict_result)\n",
    "\n",
    "        test_targets = test_targets.cpu()\n",
    "        predict_result = predict_result.cpu()\n",
    "\n",
    "        for i in range(test_targets.size(0)):\n",
    "            # 绘制真实数据\n",
    "            plt.plot(\n",
    "                range(test_targets.size(1)),\n",
    "                test_targets[i, :, -1],\n",
    "                label=\"True Data\",\n",
    "                color=\"blue\",\n",
    "            )\n",
    "\n",
    "            # 绘制预测数据\n",
    "            plt.plot(\n",
    "                range(test_targets.size(1)),\n",
    "                predict_result[i, :, -1],\n",
    "                label=\"Predicted Data\",\n",
    "                color=\"red\",\n",
    "            )\n",
    "\n",
    "            # 添加标签和标题\n",
    "            plt.xlabel(\"Time Steps\")\n",
    "            plt.ylabel(\"Values\")\n",
    "            plt.title(\"Comparison of Predicted and True Data\")\n",
    "\n",
    "            # 添加图例\n",
    "            plt.legend()\n",
    "\n",
    "            # 显示图\n",
    "            plt.show()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
